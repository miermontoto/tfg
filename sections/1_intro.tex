\chapter{Introducción}\label{chap:intro}
En este capítulo se presenta una introducción al trabajo de desarrollo de software realizado,
proporcionando un contexto general y estableciendo el escenario para los capítulos siguientes.
Se discutirán los antecedentes y la motivación detrás de este trabajo, la finalidad del proyecto,
y se proporcionará una breve descripción de la empresa en la que se ha desarrollado este trabajo.
Este capítulo tiene como objetivo proporcionar una visión general del proyecto y establecer las
bases para los capítulos detallados que siguen.

\section{Antecedentes}\label{sec:antecedentes}
Hoy en día, nos encontramos en una era donde la generación y almacenamiento de datos crece
exponencialmente\footnote{\url{https://www.statista.com/statistics/871513/worldwide-data-created/}},
reflejando una realidad ineludible en el ámbito empresarial. La diversidad de fuentes y formatos de
estos datos introduce una complejidad significativa en su manejo, conocida como \textit{heterogeneidad}
\footnote{\url{https://www.sciencedirect.com/topics/computer-science/data-heterogeneity}}, siendo las
bases de datos, archivos de registros y APIs las fuentes más habituales.

La era del \textit{big data} describe este fenómeno de acumulación masiva de datos, cuya magnitud y
complejidad sobrepasan las capacidades de los métodos de procesamiento convencionales. Se distingue
por tres características principales: volumen, variedad y velocidad. Su adecuada gestión y análisis
pueden otorgar ventajas competitivas significativas a las empresas, tales como el descubrimiento de
patrones ocultos, identificación de nuevas oportunidades de mercado y optimización de procesos de toma
de decisiones.

La evolución tecnológica ha propiciado el desarrollo de innovadoras herramientas y metodologías
diseñadas para enfrentar estos desafíos. Entre ellas, los \textit{data lakes} (o \emph{lagos de
información}) se destacan por su capacidad para consolidar vastos volúmenes de datos heterogéneos,
facilitando su posterior análisis y aprovechamiento de manera más efectiva.

Sin embargo, el proceso de integración, visualización y análisis de estos datos es una tarea desafiante,
ya que requiere de una gran cantidad de recursos y de un tiempo de desarrollo considerable del que,
normalmente, no se dispone en el ámbito empresarial.

\newpage{}
\subsection{Análisis y visualización de datos}\label{subsec:datos}
Con la ingesta masiva de datos, se presentan nuevos problemas a la hora de analizar y obtener
información de ellos:

\begin{itemize}
	\item Sin la necesaria automatización y correcta aplicación de los procesos ETL,
		el proceso de análisis puede dar lugar a errores y decisiones de negocio incorrectas
		al tratarse del crecimiento exponencial de los datos.
	\item La heterogeneidad de los datos, tanto en formato como en origen, dificulta su
		consolidación y análisis.
	\item La masificación de información a representar requiere de herramientas de visualización,
		como \textit{dashboards}, que permitan a su análisis.
\end{itemize}

La visualización de datos es una técnica que permite representar la información de manera visual,
para facilitar su análisis y comprensión. La visualización de datos es una parte importante del
proceso de análisis de datos, ya que permite identificar patrones, tendencias y anomalías en los
datos de forma más rápida y sencilla.

La evolución de la visualización de datos ha ido de la mano de la evolución de la tecnología, y
actualmente existen múltiples herramientas y técnicas que permiten analizar datos de forma
más eficiente y efectiva. Una de estas técnicas es el modelo DIKW.

La pirámide DIKW\cite{enwiki:1211227190} es un modelo que describe la relación entre los datos,
la información, el conocimiento y la sabiduría. Según este modelo, los datos son la materia prima
de la información, que a su vez es la materia prima del conocimiento, que a su vez es la materia
prima de la sabiduría.

\newpage{}
\section{Motivación}\label{sec:motivacion}
El proyecto surge de la necesidad de la empresa de extraer información y conocimiento de las
múltiples y heterogéneas fuentes de datos de las que se disponen, tanto internas (e.g.~bases de
datos, archivos de registros, APIs, entre otros), como externas (e.g. APIs o datos de webs de
terceros, datos de fuentes públicas\ldots).

En la actualidad, la empresa dispone de una gran cantidad de datos que se encuentran en
diferentes formatos y en diferentes ubicaciones, lo que dificulta su análisis y explotación.
Por otra parte, se depende de la consulta manual o de servicios de terceros para poder analizar
estos datos, lo que supone un coste adicional.

Además del uso interno, la empresa también quiere ofrecer a sus clientes la posibilidad de
consultar estos datos de forma visual y sencilla, para que puedan analizarlos y explotarlos de
forma autónoma, lo que supondría un valor añadido para los mismos.

\section{Finalidad del proyecto}\label{sec:finalidad}
\subsection{Objetivos}\label{subsec:objetivos}
El objetivo de este proyecto es la creación de una herramienta que centralice y unifique las
fuentes de datos heterogéneas. Esta consulta se realiza actualmente de manera manual mediante
la creación y despliegue de un data lake (y todas las herramientas asociadas necesarias), con
la finalidad de analizar los datos de forma más eficiente. El cumplimiento de este objetivo
permitirá a la empresa obtener una serie de beneficios;

\begin{itemize}
	\item \textbf{Eliminar el tiempo invertido} en la consulta manual de los datos.
	\item \textbf{Reducir los costes} asociados a servicios de terceros.
	\item \textbf{Mejorar la toma de decisiones}, al poder analizar los datos de forma más eficiente.
	\item \textbf{Incrementar la calidad de los servicios} ofrecidos a los clientes, al poder ofrecerles
	    la posibilidad de consultar los datos de forma visual y sencilla.
	\item \textbf{Explotar económicamente} este servicio, ofreciéndolo a terceros.
\end{itemize}

Además de la mejora de los procesos ya existentes, la explotación mediante esta herramienta
abrirá la puerta a nuevas posibilidades de análisis y explotación de los datos, como la detección
de anomalías en la infraestructura o la predicción de patrones y eventos futuros.

\subsection{Requisitos}\label{subsec:requisitos}
Los anteriores objetivos se detallan en requisitos funcionales y no funcionales, que se
presentan a continuación.

\subsubsection{Requisitos funcionales}\label{subsubsec:funcionales}
\begin{enumerate}[label=RF\arabic*.]
	\item \textbf{Integración de fuentes de datos:} El data lake debe ser capaz de integrar datos
		de diferentes fuentes, de manera que se almacenen de manera unificada.
		\begin{enumerate}[label*=\arabic*.]
			\item \emph{Fuentes internas} \begin{enumerate}[label*=\arabic*.]
					\item MongoDB
					\item MySQL
					\item Okticket API
				\end{enumerate}
			\item \emph{Fuentes externas} \begin{enumerate}[label*=\arabic*.]
					\item APIs de terceros
					\item Webs de terceros (\textit{scraping})
				\end{enumerate}
		\end{enumerate}
	\item \textbf{Transformación de datos:} El data lake debe ser capaz de transformar los datos
		integrados en un formato que pueda ser analizado.
	\item \textbf{Limpieza de datos:} El data lake debe ser capaz de limpiar los datos, para que
		estén preparados para el análisis. \begin{enumerate}[label*=\arabic*.]
			\item Eliminación de duplicados
			\item Eliminación de valores nulos
			\item Eliminación de valores atípicos
			\item Transformación de tipos de datos
			\item Transformación de fechas
		\end{enumerate}
	\item \textbf{Gestón de metadatos:} El sistema debe gestionar metadatos, permitiendo a los
		usuarios entender el origen, el contenido y el contexto de los datos. A la hora de
		visualizar los datos, deben ser mostrados en la interfaz de consulta.
		\begin{enumerate}[label*=\arabic*.]
			\item Fuente de origen
			\item Empresa vinculada
			\item Fecha de creación
			\item Fecha de actualización
		\end{enumerate}
	\item \textbf{Soporte para análisis:} Debe permitir el análisis de datos complejos, incluyendo
		el procesamiento de grandes volúmenes de datos mediante búsquedas con el objetivo de segmentar
		la información. Deberá implementarse mediante un lenguaje de dominio específico (DSL).
	\item \textbf{Interfaz de consulta:} Debe ser capaz de visualizar los datos de forma sencilla y
		eficiente, para que los usuarios puedan analizarlos y explotarlos de forma autónoma.
		\begin{enumerate}[label*=\arabic*.]
			\item \emph{Interfaz interna}
				\begin{enumerate}[label*=\arabic*.]
					\item Monitorización de infraestructura
					\item Análisis de negocio
					\item Gestión de soporte
				\end{enumerate}
			\item \emph{Intefaz externa:} Consulta de datos a nivel externo para los administradores de
				empresas cliente que ofrezca un análisis a nivel de negocio de la información que se
				posea relacionada con dicha empresa.
		\end{enumerate}
	\item \textbf{Gestión de calidad:} Herramientas y procesos para monitorear y mejorar continuamente
		la calidad de los datos almacenados en el datalake. Debe soportar la generación de informes de
		calidad de datos y la configuración de alertas basadas en umbrales de calidad de datos.
\end{enumerate}

\subsubsection{Requisitos no funcionales}\label{subsubsec:nofuncionales}
\begin{enumerate}[label=RNF\arabic*.]
	\item \textbf{Automatización} \begin{enumerate}[label*=\arabic*.]
		\item Los procesos ETL que alimentan el \textit{data lake} deben ser automáticos.
		\item Los procesos de limpieza y transformación de datos deben ser automáticos.
		\item Los procesos de gestión de metadatos deben ser automáticos.
		\item El proceso de despliegue del programa debe estar automatizado (pipelines, docker/kubernetes, etc.)
	\end{enumerate}
	\item \textbf{Escalabilidad} \begin{enumerate}[label*=\arabic*.]
		\item El sistema debe ser escalable horizontalmente, para que pueda soportar el crecimiento de los datos.
		\item El sistema debe ser escalable verticalmente, para que pueda soportar el crecimiento de los recursos de procesamiento.
	\end{enumerate}
	\item \textbf{Disponibilidad} \begin{enumerate}[label*=\arabic*.]
		\item El sistema debe ser disponible 24/7, para que pueda ser utilizado por los usuarios en cualquier momento.
		\item El sistema debe contar con al menos 3 nueves de disponibilidad (99,9\%).
	\end{enumerate}
	\item \textbf{Seguridad de la información:} El sistema debe garantizar la protección de la información y asegurar que los datos
		no puedan ser accedidos por ningún agente externo.
	\item \textbf{Rendimiento} \begin{enumerate}[label*=\arabic*.]
		\item La latencia de navegación entre el usuario y el sistema debe ser inferior a 2 segundos.
		\item La latencia de consultas de datos debe ser inferior a 15 segundos.
	\end{enumerate}
	\item \textbf{Eficiencia de costes:} El sistema debe ser eficiente en el coste de desarrollo, tanto en el tiempo como en el coste de
		recursos.
	\item \textbf{Mantenibilidad:} el sistema y su código debe de ser mantenible e introducir buenas prácticas de manera
		que se facilite el desarrollo futuro sobre el mismo.
	\item \textbf{Cumplimiento normativo y privacidad:} El sistema debe cumplir con toda la legislación y normativa
		relativa a la privacidad de los datos, incluyendo la normativa de la empresa y las certificaciones a la que
		está sometida.
\end{enumerate}

\newpage{}
\section{La empresa}\label{sec:empresa}
Okticket es una startup nacida en Gijón en 2017 cuyo producto principal es un servicio software que escanea
automáticamente de tickets y notas de gastos lo que permite reducir los costes y el tiempo que invierten
las empresas en contabilizar y manejar los gastos de viaje de los profesionales.

La empresa tienen su suede principal en el Parque Tecnológico de Gijón, aunque cuenta con un número
de sedes creciente en varios países, como Francia, Portugal o, más recientemente, México. En esta
oficina principal se encuentran los departamentos de ventas y marketing, así como el equipo de
desarrollo y soporte.

Okticket es una de las empresas que más crecen tanto del sector como del propio Parque
Tecnológico. Debido a este rápido crecimiento, el equipo está en constante desarrollo y
cambio, tanto aquí en España como en el resto de sedes. Este crecimiento se refleja
en la recepción de un gran número de galardones y reconocimientos.
\footnote{\href{https://www.linkedin.com/posts/okticket_okticket-en-el-especial-startups-de-forbes-activity-7140622980618903552-UGWK}{Okticket en el especial startups 2023 de Forbes (LinkedIn)}}
\footnote{\href{https://www.elcomercio.es/economia/arcelor-okticket-premios-20230222002438-ntvo.html}{Arcelor y Okticket, premios nacional de Ingeniería Informática (EL COMERCIO)}}
\footnote{\href{https://www.okticket.es/blog/empresa-pyme-innovadora}{Okticket recibe el sello Pyme Innovadora (okticket.es)}}
\footnote{\href{https://www.okticket.es/blog/okticket-empresa-emergente-certificada}{Okticket, empresa emergente certificada (okticket.es)}}

La parte principal del negocio es el núcleo del software como servicio (Software as a
Service en inglés, en adelante \textit{SaaS}), es decir, la aplicación completa tanto
para administradores como para empleados. Este SaaS se oferta a empresas de cualquier
tamaño, cuyo precio final varía en función del número de usuarios, las características
e integraciones que requiera la empresa cliente y el soporte que se ofrezca.

Recientemente se han añadido nuevas propuestas a la cartera de servicios ofertada por
Okticket, como la OKTCard {-} una tarjeta inteligente que gestiona automáticamente los gastos,
así como la inclusión de nuevos ``módulos'' de gestión de gastos y viajes.

Debido a todo este crecimiento, la empresa maneja una gran cantidad de datos importantes que se
encuentran en diferentes formatos y en diferentes ubicaciones, lo que dificulta su análisis y
explotación. Por otra parte, se depende de la consulta manual o de servicios de terceros para
poder analizar estos datos, lo que supone un coste adicional.
